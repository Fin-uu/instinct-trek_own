// server.js - å¾Œç«¯ API æœå‹™å™¨
import express from 'express';
import OpenAI from 'openai';
import cors from 'cors';

const app = express();

// ä¸­é–“ä»¶
app.use(cors()); // å…è¨±è·¨åŸŸè«‹æ±‚
app.use(express.json()); // è§£æ JSON

// é€£æ¥åˆ° vLLM æœå‹™å™¨
const openai = new OpenAI({
  baseURL: 'http://210.61.209.139:45014/v1', // vLLM æœå‹™å™¨åœ°å€
  apiKey: 'dummy-key',
  defaultHeaders: {
    'Content-Type': 'application/json; charset=utf-8',
    'Accept-Charset': 'utf-8'
  }
});

// System prompt (define AI role)
const SYSTEM_PROMPT = `You are TravelMate, a professional and friendly travel guide. Provide helpful travel advice, attraction recommendations, and trip planning assistance. Keep responses clear, specific, and practical.`;

// Helper function to clean vLLM response
function cleanVLLMResponse(content) {
  if (!content) return '';
  
  // vLLM sometimes returns responses with "analysis...assistantfinal" prefix
  // Extract only the final assistant response
  const finalMarker = 'assistantfinal';
  const finalIndex = content.indexOf(finalMarker);
  
  if (finalIndex !== -1) {
    return content.substring(finalIndex + finalMarker.length).trim();
  }
  
  // If no marker found, return original content
  return content;
}

// ==================== API ç«¯é» ====================

// 1. å¥åº·æª¢æŸ¥
app.get('/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    message: 'AI Travel Guide Backend Running',
    vllm: 'http://210.61.209.139:45014/v1'
  });
});

// 2. Basic chat endpoint
app.post('/api/chat', async (req, res) => {
  try {
    const { message, history = [] } = req.body;

    console.log('ğŸ“¨ Received message:', message);
    console.log('ğŸ“š History length:', history.length);
    if (history.length > 0) {
      console.log('ğŸ“š Last history item:', history[history.length - 1]);
    }

    // Build conversation history
    const messages = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...history.map(msg => ({
        role: msg.role,
        content: msg.content
      })),
      { role: 'user', content: message }
    ];

    // Call vLLM using native fetch with explicit UTF-8 encoding
    const requestBody = {
      model: 'openai/gpt-oss-120b',
      messages: messages,
      max_tokens: 1500,
      temperature: 0.8,
    };
    
    console.log('ğŸ“¤ Sending to vLLM:', JSON.stringify(requestBody).substring(0, 200));
    
    const vllmResponse = await fetch('http://210.61.209.139:45014/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json; charset=utf-8',
        'Accept': 'application/json',
      },
      body: JSON.stringify(requestBody)
    });

    if (!vllmResponse.ok) {
      throw new Error(`vLLM returned ${vllmResponse.status}: ${vllmResponse.statusText}`);
    }

    const completion = await vllmResponse.json();
    let response = completion.choices[0].message.content;
    
    // Clean the response to remove vLLM's internal markers
    response = cleanVLLMResponse(response);
    
    console.log('âœ… AI response:', response.substring(0, 100) + '...');

    res.json({
      success: true,
      content: response,
      usage: completion.usage
    });

  } catch (error) {
    console.error('âŒ Chat error:', error.message);
    res.status(500).json({
      success: false,
      error: 'Cannot process request',
      details: error.message
    });
  }
});

// 3. æµå¼å°è©±ç«¯é»ï¼ˆé€å­—é¡¯ç¤ºï¼‰
app.post('/api/chat/stream', async (req, res) => {
  try {
    const { message, history = [] } = req.body;

    console.log('ğŸ“¨ æ”¶åˆ°æµå¼è«‹æ±‚:', message);

    const messages = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...history.map(msg => ({
        role: msg.role,
        content: msg.content
      })),
      { role: 'user', content: message }
    ];

    // è¨­ç½® SSE æ¨™é ­
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    // æµå¼å‘¼å«
    const stream = await openai.chat.completions.create({
      model: 'openai/gpt-oss-120b',
      messages: messages,
      max_tokens: 1500,
      temperature: 0.8,
      stream: true
    });

    // é€å¡Šç™¼é€
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        res.write(`data: ${JSON.stringify({ content })}\n\n`);
      }
    }

    res.write('data: [DONE]\n\n');
    res.end();

    console.log('âœ… æµå¼å›æ‡‰å®Œæˆ');

  } catch (error) {
    console.error('âŒ Stream chat error:', error.message);
    res.status(500).json({
      success: false,
      error: 'Cannot process stream request',
      details: error.message
    });
  }
});

// 4. æ™ºæ…§è¡Œç¨‹ç”Ÿæˆç«¯é»
app.post('/api/generate-itinerary', async (req, res) => {
  try {
    const { destination, days, preferences = {} } = req.body;

    console.log('ğŸ“¨ ç”Ÿæˆè¡Œç¨‹:', { destination, days });

    const prompt = `
è«‹ç‚ºä»¥ä¸‹æ—…éŠéœ€æ±‚ç”Ÿæˆè©³ç´°è¡Œç¨‹ï¼š

ç›®çš„åœ°ï¼š${destination}
å¤©æ•¸ï¼š${days}å¤©
åå¥½ï¼š${JSON.stringify(preferences)}

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "title": "è¡Œç¨‹æ¨™é¡Œ",
  "style": "æ—…éŠé¢¨æ ¼æè¿°",
  "crowd": "äººæµå»ºè­°",
  "budget": "é ç®—ç¯„åœ",
  "steps": "æ¯æ—¥å¹³å‡æ­¥æ•¸",
  "highlights": "ç²¾é¸æ™¯é»ï¼ˆç”¨é “è™Ÿåˆ†éš”ï¼‰",
  "dailySchedule": [
    {
      "date": "ç¬¬ 1 å¤©",
      "totalSteps": "8000æ­¥",
      "totalCost": "NT$2000",
      "totalTime": "8å°æ™‚",
      "activities": [
        {
          "time": "09:00",
          "name": "æ™¯é»åç¨±",
          "type": "temple",
          "description": "è©³ç´°æè¿°ï¼ŒåŒ…å«ç‰¹è‰²å’Œæ³¨æ„äº‹é …",
          "duration": "1.5å°æ™‚",
          "cost": "Â¥500",
          "transport": "äº¤é€šæ–¹å¼å’Œæ™‚é–“"
        }
      ]
    }
  ]
}

é‡è¦è¦æ±‚ï¼š
1. activities çš„ type åªèƒ½æ˜¯ï¼štemple, food, shopping, cafe, sightseeing, nature
2. æ¯å¤©å®‰æ’ 3-5 å€‹æ´»å‹•
3. è€ƒæ…®äº¤é€šæ™‚é–“å’Œç”¨é¤æ™‚é–“
4. è²»ç”¨ä½¿ç”¨ç•¶åœ°è²¨å¹£
5. ç¢ºä¿è¿”å›æœ‰æ•ˆçš„ JSONï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰
`;

    const completion = await openai.chat.completions.create({
      model: 'openai/gpt-oss-120b',
      messages: [
        { role: 'system', content: 'ä½ æ˜¯å°ˆæ¥­çš„æ—…éŠè¦åŠƒå¸«ï¼Œæ“…é•·ç”Ÿæˆçµæ§‹åŒ–çš„è¡Œç¨‹è³‡æ–™ã€‚' },
        { role: 'user', content: prompt }
      ],
      max_tokens: 3000,
      temperature: 0.7
    });

    let response = completion.choices[0].message.content;
    response = cleanVLLMResponse(response);
    
    // æå– JSON
    let itinerary;
    try {
      // å˜—è©¦ç›´æ¥è§£æ
      itinerary = JSON.parse(response);
    } catch {
      // å¦‚æœå¤±æ•—ï¼Œå˜—è©¦æå– JSON å¡Š
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        itinerary = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('ç„¡æ³•è§£æ JSON');
      }
    }

    console.log('âœ… è¡Œç¨‹ç”ŸæˆæˆåŠŸ:', itinerary.title);

    res.json({
      success: true,
      itinerary: itinerary
    });

  } catch (error) {
    console.error('âŒ è¡Œç¨‹ç”ŸæˆéŒ¯èª¤:', error.message);
    res.status(500).json({
      success: false,
      error: 'Cannot generate itinerary',
      details: error.message
    });
  }
});

// 5. æ¸¬è©¦ vLLM é€£æ¥
app.get('/api/test-vllm', async (req, res) => {
  try {
    // æ¸¬è©¦ç°¡å–®å°è©±
    const testResponse = await openai.chat.completions.create({
      model: 'openai/gpt-oss-120b',
      messages: [
        { role: 'user', content: 'Hello, please introduce yourself in one sentence.' }
      ],
      max_tokens: 100
    });

    const cleanedResponse = cleanVLLMResponse(testResponse.choices[0].message.content);

    res.json({
      success: true,
      message: 'vLLM connected successfully!',
      response: cleanedResponse,
      model: 'openai/gpt-oss-120b'
    });

  } catch (error) {
    res.status(500).json({
      success: false,
      message: 'vLLM é€£æ¥å¤±æ•—',
      error: error.message
    });
  }
});

// å•Ÿå‹•æœå‹™å™¨
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘   ğŸš€ AI Travel Guide Backend Started â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('');
  console.log(`âœ… Server running at: http://localhost:${PORT}`);
  console.log(`ğŸ“¡ Connected to vLLM: http://210.61.209.139:45014/v1`);
  console.log('');
  console.log('Available endpoints:');
  console.log(`  GET  /health                 - Health check`);
  console.log(`  GET  /api/test-vllm          - Test vLLM connection`);
  console.log(`  POST /api/chat               - Basic chat`);
  console.log(`  POST /api/chat/stream        - Streaming chat`);
  console.log(`  POST /api/generate-itinerary - Generate itinerary`);
  console.log('');
  console.log('Press Ctrl+C to stop server');
  console.log('');
});

// export default app; // è¨»è§£æ‰ä»¥ä¿æŒæœå‹™å™¨é‹è¡Œ
